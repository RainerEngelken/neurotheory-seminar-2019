{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we will fit a static dimensionality reduction model (factor analysis, FA) and dynamic dimensionality reduction model (a latent dynamical system, LDS) to some neural data. We will use the same neural data provided for the encoding and decoding portion of the course (courtesy of ? via Josh), which is available here:\n",
    "https://www.dropbox.com/s/jcief15oql3tkll/example_data_m1.pickle?dl=0 \n",
    "\n",
    "<br>\n",
    "\n",
    "First run \"easy_install numpy scipy pykalman\" in the terminal.\n",
    "\n",
    "As before, much of the notebook is already filled in, and you should provide code in the empty cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "The dataset is a recording of neurons in motor cortex, while a monkey is reaching to targets. Previously we fit encoding and decoding models between the x/y velocities of the movement and the neural activity (both of which have been put into 50 ms time bins). This time we will ignore the velocities and try to make sense of this data by just using the neural recordings.\n",
    "\n",
    "\"neural_data\" is a matrix of size \"number of time bins\" x \"number of neurons\", where each entry is the firing rate of a given neuron in a given time bin\n",
    "\n",
    "\"vels_binned\" is a matrix of size \"number of time bins\" x 2, where the columns are the x and y velocities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in the cell below, enter the folder your data is in. For example, if it is in your current directory: <br>\n",
    "folder=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you'll need to change what is commented/uncommented below if you are running python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder+'example_data_m1.pickle','rb') as f:\n",
    "    neural_data,vels_binned=pickle.load(f,encoding='latin1') #If using python 3\n",
    "#     neural_data,vels_binned=pickle.load(f) #If using python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refer to the neural_data as \"Y\" (the observations) and we will not use the velocities for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.copy(neural_data)\n",
    "X=np.copy(vels_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data - split into training/testing sets\n",
    "We will fit the models on the training set, and then check its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set what part of data should be part of the training/testing/validation sets\n",
    "training_range=[0, 0.7]\n",
    "testing_range=[0.7, 1]\n",
    "\n",
    "num_examples=X.shape[0]\n",
    "\n",
    "#Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "#This makes it so that the different sets don't include overlapping neural data\n",
    "training_set=np.arange(np.int(np.round(training_range[0]*num_examples)),np.int(np.round(training_range[1]*num_examples)))\n",
    "testing_set=np.arange(np.int(np.round(testing_range[0]*num_examples)),np.int(np.round(testing_range[1]*num_examples)))\n",
    "\n",
    "#Get training data\n",
    "Y_train=Y[training_set,:]\n",
    "X_train=X[training_set,:]\n",
    "\n",
    "#Get testing data\n",
    "Y_test=Y[testing_set,:]\n",
    "X_test=X[testing_set,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Static dimensionality reduction - Factor Analysis (FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import factor analysis function (isn't this easy and fun??)\n",
    "from sklearn.decomposition import FactorAnalysis as FA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by fitting a model with two components. The FA documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the observations from the training into the 2D latent space found by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Do two components (latent variables) actually fit the data well? As mentioned in class, one of the difficulties of using dimensionality reduction methods is deciding how many components to use. Let's loop over the number of components from 1 to your favorite value of N (maybe start with a small favorite value of N, like 10), calculating the average log-likelihood of both the training samples and testing samples (using FA.score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the log-likelihoods as a function of number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly seen feature of neural activity is that the variance increases with the mean firing rate. Variance is typically estimated by using multiple repeats of the same trial; however, we don't have that type of information with this data. FA allows us to estimate the variance of the firing rate without repeats of the same trial (something we don't get from PCA).\n",
    "<br>\n",
    "\n",
    "Using your favorite number of components given the results above, fit FA, and plot the estimated variance (found in your_fa_model.noise_variance_) as a function of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned in class that FA is preferrable over PPCA for fitting neural data because PPCA makes the assumption that the noise variance of each neuron is identical. However, you (should have seen above) that this is not the case with this data. Let's compare FA and PPCA to see if FA is indeed a better model.\n",
    "<br>\n",
    "\n",
    "To do so we're going to use a model selection function that comes in scikitlearn called `cross_val_score`, which will do the dirty work of splitting the data into train/test folds, fitting the desired models, _and_ returns cross-validated log-likelihoods (which is calculated for PCA using the PPCA likelihood function; see PCA.noise_variance_ in the scikitlearn documentation for more info).\n",
    "<br>\n",
    "\n",
    "The following useful function is adapted from [this scikitlearn example](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare FA and PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def compute_scores(Y, max_num_components):\n",
    "    pca = PCA(svd_solver='full')\n",
    "    fa = FA()\n",
    "\n",
    "    pca_scores, fa_scores = [], []\n",
    "    for n in range(max_num_components):\n",
    "        print('fitting %i components' % (n + 1))\n",
    "        pca.n_components = n + 1\n",
    "        fa.n_components = n\n",
    "        pca_scores.append(np.mean(cross_val_score(pca, Y, cv=5)))\n",
    "        fa_scores.append(np.mean(cross_val_score(fa, Y, cv=5)))\n",
    "\n",
    "    return pca_scores, fa_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `compute_scores` function for a given number of components, and plot the resulting scores (log-likelihoods). This may take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic dimensionality reduction - LDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We are going to run our inference of linear dynamical systems using a python package called pykalman, which implements EM for LDS. \n",
    " <br>\n",
    " \n",
    " Make sure you have run \"easy_install numpy scipy pykalman\" in terminal. You may have to reload this notebook after having done that. See [here](http://pykalman.github.io/) for documentation. The documentation goes over a lot of practical tidbits about the estimation of the relevant parameters - you should definitely browse through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating likelihoods - take a look if interested\n",
    "from scipy.stats import multivariate_normal\n",
    "def compute_llh(kf,observations):\n",
    "    (filtered_state_means, filtered_state_covariances) = kf.smooth(observations)\n",
    "    n_timesteps = observations.shape[0]\n",
    "    loglikelihoods = np.zeros(n_timesteps)\n",
    "    for t in range(n_timesteps):\n",
    "        observation = observations[t]\n",
    "        observation_matrix = kf.observation_matrices\n",
    "        observation_offset = kf.observation_offsets\n",
    "        observation_covariance = kf.observation_covariance\n",
    "        predicted_state_mean = filtered_state_means[t]\n",
    "        predicted_state_covariance = filtered_state_covariances[t]\n",
    "        \n",
    "        predicted_observation_mean = np.dot(observation_matrix,predicted_state_mean).T + observation_offset\n",
    "        predicted_observation_covariance = (\n",
    "            np.dot(observation_matrix,\n",
    "                   np.dot(predicted_state_covariance,\n",
    "                          observation_matrix.T))\n",
    "            + observation_covariance\n",
    "        )\n",
    "\n",
    "        loglikelihoods[t] = np.log(multivariate_normal.pdf(\n",
    "            observation,\n",
    "            mean=predicted_observation_mean,\n",
    "            cov=predicted_observation_covariance\n",
    "        ))\n",
    "    return sum(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for fitting LDS and predict the state and smoothed observations\n",
    "# The docs may be useful here: http://pykalman.github.io/. Feel free to sneak a peak under the hood - it is exactly as discussed in lecture.\n",
    "\n",
    "def fit_LDS(Y_train,Y_test,latentdim=2,n_iter=5,smoothorfilt='smooth'):\n",
    "    # Initialize a Kalman Filter class, while specifying the number of latent dimensions you want.\n",
    "    kf = KalmanFilter(initial_state_mean=np.zeros((latentdim)), n_dim_obs=Y_train.shape[1]) ##\n",
    "    # Estimate the states and the transition + observations matrices using EM, while specifying the number of iterations.\n",
    "    kf.em(Y_train, n_iter=n_iter) ##\n",
    "    \n",
    "    llh={}; pred_state={}; smoothed_obs={};\n",
    "    llh['train'] = compute_llh(kf,Y_train)\n",
    "    llh['test'] = compute_llh(kf,Y_test)\n",
    "    \n",
    "    if smoothorfilt=='smooth':\n",
    "        # Obtain the predicted state mean and covariances using a kalman *smoother*, for both train and test data.\n",
    "        # Store the predicted state mean in pred_state['train'] and pred_state['test'] for train and test data.\n",
    "        (pred_state['train'], filtered_state_covariances) = kf.smooth(Y_train) ##\n",
    "        (pred_state['test'], smoothed_state_covariances) = kf.smooth(Y_test) ##\n",
    "    elif smoothorfilt=='filt':\n",
    "        # Obtain the predicted state mean and covariances using a kalman *filter*, for both train and test data.\n",
    "        # Store the predicted state mean in pred_state['train'] and pred_state['test'] for train and test data.\n",
    "        (pred_state['train'], filtered_state_covariances) = kf.filter(Y_train) ##\n",
    "        (pred_state['test'], smoothed_state_covariances) = kf.filter(Y_test) ##\n",
    "\n",
    "    # Obtain the mean smoothed observations using the LDS forward / generative model\n",
    "    # Store the mean smoothed observations in smoothed_obs['train'] and smoothed_obs['test'] for train and test data.\n",
    "    smoothed_obs['train'] = np.dot(kf.observation_matrices,pred_state['train'].T).T + kf.observation_offsets ##\n",
    "    smoothed_obs['test'] = np.dot(kf.observation_matrices,pred_state['test'].T).T + kf.observation_offsets ##\n",
    "\n",
    "    return (llh,pred_state,smoothed_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a linear decoding model from Y_train to X_train based on the training data, and returns the R^2 on test data. \n",
    "def fit_linear_model(Y_train,X_train,Y_test,X_test):\n",
    "    reg = sklearn.linear_model.LinearRegression() ##\n",
    "    reg.fit(Y_train, X_train) ## \n",
    "    return reg.score(Y_test, X_test) ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "\n",
    "# This is an example of running the kalman filter where, knowing the transition and observation matrices, \n",
    "# you estimate the state. Take a look if you want to see an example and play around with transition and\n",
    "# observation matrices, but it is not important for this tutorial.\n",
    "kf = KalmanFilter(transition_matrices = [[1, 1], [0, 1]], observation_matrices = [[0.1, 0.5], [-0.3, 0.0]])\n",
    "measurements = np.asarray([[1,0], [0,0], [0,1]])\n",
    "kf = kf.em(measurements, n_iter=15)\n",
    "llh = compute_llh(kf,measurements)\n",
    "(filtered_state_means, filtered_state_covariances) = kf.filter(measurements)\n",
    "(smoothed_state_means, smoothed_state_covariances) = kf.smooth(measurements)\n",
    "\n",
    "filtered_measurements=np.dot(kf.observation_matrices,filtered_state_means.T).T + kf.observation_offsets\n",
    "smoothed_measurements=np.dot(kf.observation_matrices,smoothed_state_means.T).T + kf.observation_offsets\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(measurements[:,0],measurements[:,1],'.', label='Observations')\n",
    "plt.plot(filtered_measurements[:,0],filtered_measurements[:,1],'.-', label='Filtered Observations')\n",
    "plt.plot(smoothed_measurements[:,0],smoothed_measurements[:,1],'.-', label='Smoothed Observations')\n",
    "plt.legend()\n",
    "plt.title('llh='+str(llh))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of running the EM algorithm where you first estimate the transition and observation matrices,\n",
    "# then you estimate the state.\n",
    "n_iter=5\n",
    "\n",
    "# Consider one latent dimension\n",
    "llh_l1,pred_state,smoothed_obs_l1=fit_LDS(measurements,measurements,latentdim=1,n_iter=n_iter)\n",
    "plt.figure()\n",
    "plt.plot(measurements[:,0],measurements[:,1],'.-', label='Observations')\n",
    "plt.plot(smoothed_obs_l1['train'][:,0],smoothed_obs_l1['train'][:,1],'.-', label='Smoothed Observations')\n",
    "plt.legend()\n",
    "plt.title('Model with 1 latent, llh='+str(llh_l1['train']))\n",
    "plt.show()\n",
    "\n",
    "# Consider two latent dimensions. Remember, our observations are dimension 2.\n",
    "llh_l2,pred_state,smoothed_obs_l2=fit_LDS(measurements,measurements,latentdim=2,n_iter=n_iter)\n",
    "plt.figure()\n",
    "plt.plot(measurements[:,0],measurements[:,1],'.-', label='Observations')\n",
    "plt.plot(smoothed_obs_l2['train'][:,0],smoothed_obs_l2['train'][:,1],'.-', label='Smoothed Observations')\n",
    "plt.legend()\n",
    "plt.title('Model with 2 latents, llh='+str(llh_l2['train']))\n",
    "plt.show()\n",
    "\n",
    "# Let's iterate over the number of latents to get an idea of what's happening as we increase this.\n",
    "llh_latents=[fit_LDS(measurements,measurements,latentdim=i,n_iter=n_iter)[0]['train'] for i in np.arange(10)]\n",
    "plt.figure()\n",
    "plt.plot(np.arange(10),llh_latents,'-o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question: Why does the model with one latent do so poorly? What happens after number of latents > number of observations, and why?*\n",
    "<br>\n",
    "\n",
    "*Question: What happens as you increase the number of iterations? Why? (Read the docs if you want to find out more)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to get an optimal number of latent states for LDS. We will base our decision on two factors (a) a plateau or decrease in test R^2, (b) ability to decode the velocity from the latents. \n",
    "<br>\n",
    "\n",
    "*Question: Can the test R^2 theoretically decrease for LDS as the latent dimension increases? How about PCA and FA?*\n",
    "\n",
    "<br>\n",
    "We subsample the neurons and timepoints because EM takes a long time to run. Feel free to try higher numbers of neurons if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdim=20; maxT=1000\n",
    "neurons=np.random.choice(Y_train.shape[1], obsdim, replace=False)\n",
    "\n",
    "Y_train_subset=Y_train[:maxT,neurons]\n",
    "Y_test_subset=Y_test[:,neurons]\n",
    "\n",
    "# get rid of silent neurons\n",
    "silent_indxs = np.where(np.sum(Y_train_subset, axis=0)==0)[0]\n",
    "Y_train_subset=np.delete(Y_train_subset, silent_indxs, 1)\n",
    "Y_test_subset=np.delete(Y_test_subset, silent_indxs, 1)\n",
    "\n",
    "print(Y_train_subset.shape); print(Y_test_subset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over the number of latent dimension, while storing all our results. Choose a maximum latent dimension and a latent step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "import sklearn\n",
    "\n",
    "maxlatentdim=20; latentstep=3; latentvec=np.arange(1,maxlatentdim,latentstep)\n",
    "\n",
    "trainstate, teststate, trainobs, testobs, trainllh, testllh, trainr2, testr2 = [], [], [], [], [], [], [], []\n",
    "for latentdim in latentvec:\n",
    "    print('fitting %i latent dimensions' % latentdim)\n",
    "    llh,pred_state,smoothed_obs = fit_LDS(Y_train_subset,Y_test_subset,latentdim=latentdim)\n",
    "    trainstate.append(pred_state['train']); teststate.append(pred_state['test'])\n",
    "    trainobs.append(smoothed_obs['train']); testobs.append(smoothed_obs['test'])\n",
    "    trainllh.append(llh['train']); testllh.append(llh['test'])\n",
    "    trainr2.append(sklearn.metrics.r2_score(Y_train_subset,smoothed_obs['train']))\n",
    "    testr2.append(sklearn.metrics.r2_score(Y_test_subset,smoothed_obs['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the log likelihood and R^2 as a function of the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the x and y velocities from \n",
    "\n",
    "- raw data\n",
    "- latent states of LDS\n",
    "- smoothed observations of LDS\n",
    "- PCA latents\n",
    "- FA latents\n",
    "<br>\n",
    "\n",
    "*Question: Do you expect the decoding accuracy to be the same from the latents and the smoothed observations? Why or why not?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subset=X_train[:maxT,:]\n",
    "X_test_subset=X_test[:,:]\n",
    "\n",
    "# for the x and y velocity as outputs\n",
    "for xdim in np.arange(X_test_subset.shape[1]):\n",
    "    # fit a linear decoder from all neurons to x/y velocity. Call the test R^2 r2_allobs.\n",
    "        \n",
    "    # fit a linear decoder from the subsampled neurons to x/y velocity. Call the test R^2 r2_obs.\n",
    "    \n",
    "    \n",
    "    r2_state, r2_smoothobs, r2_pca, r2_fa = [],[],[],[]\n",
    "    for latentdim in np.arange(len(trainstate)):\n",
    "        # fit a linear decoder from the estimated state to x/y velocity. Append the test R^2 to r2_state.\n",
    "        \n",
    "        # fit a linear decoder from the smoothed observations to x/y velocity. Append the test R^2 to r2_smoothobs.\n",
    "        \n",
    "        # fit a linear decoder from the PCA latents to x/y velocity. Append the test R^2 to r2_state.\n",
    "        \n",
    "        # fit a linear decoder from the FA latents to x/y velocity. Append the test R^2 to r2_state.\n",
    "        \n",
    "    # plot r2s for subsampled raw data (all/subsampled), LDS state, smoothed latents, PCA and FA compenents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question: Do you see the decoding from LDS State to be better than from the subsampled raw data at any point? Why do you think this is / is not happening?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further questions\n",
    "<br>\n",
    "(a) How does EM seem to scale with the number of neurons? What would your approach be to speed up the algorithms while recording many more neurons simultaneously?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?\n",
    "\n",
    "If you have questions on this notebook, please contact either:<br>\n",
    "Matt: m.whiteway@columbia.edu <br>\n",
    "Shreya: ss5513@columbia.edu"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
